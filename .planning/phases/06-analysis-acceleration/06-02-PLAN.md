---
phase: 06-analysis-acceleration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/parallel_analyzer.py
  - app/services/claude_service.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Multiple API calls execute concurrently instead of sequentially"
    - "Rate limiting prevents 429 errors from Anthropic API"
    - "Concurrent API call timestamps visible in logs showing overlapping execution windows"
    - "Failed batches are handled gracefully without crashing entire analysis"
    - "Progress tracking still works with parallel execution"
  artifacts:
    - path: "app/services/parallel_analyzer.py"
      provides: "ParallelAnalyzer class for async batch processing"
      exports: ["ParallelAnalyzer", "run_parallel_analysis"]
    - path: "requirements.txt"
      provides: "New dependencies for async processing"
      contains: "aiohttp"
  key_links:
    - from: "app/services/claude_service.py"
      to: "app/services/parallel_analyzer.py"
      via: "import and delegation"
      pattern: "from app.services.parallel_analyzer import"
    - from: "app/services/parallel_analyzer.py"
      to: "anthropic.AsyncAnthropic"
      via: "async client usage"
      pattern: "AsyncAnthropic"
---

<objective>
Implement parallel API call processing with rate limiting to achieve significant reduction in analysis time by processing multiple batches concurrently.

Purpose: The current sequential batch processing is the primary bottleneck. Each API call takes 10-60 seconds, and we make ~30 calls sequentially. Running 5 calls in parallel with proper rate limiting dramatically reduces total time.

Output: ParallelAnalyzer service that processes batches concurrently while respecting Anthropic rate limits.
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-analysis-acceleration/06-RESEARCH.md

Key context from research:
- Use AsyncAnthropic client from anthropic SDK (already supports async)
- Use aiolimiter for rate limiting (100 requests per minute, conservative)
- Use asyncio.Semaphore for max concurrent calls (5 concurrent)
- Progress tracking needs thread-safe updates with asyncio.Lock
- Pattern: asyncio.gather(*tasks, return_exceptions=True) for parallel execution
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add async dependencies</name>
  <files>requirements.txt</files>
  <action>
Add the required async packages to requirements.txt:

```
aiohttp>=3.9.0
aiolimiter>=1.1.0
```

Note: The anthropic SDK already includes async support via AsyncAnthropic, so no additional install needed for that.

Run pip install to ensure dependencies are available:
```bash
pip install aiohttp aiolimiter
```
  </action>
  <verify>
Run: `pip show aiolimiter` - should show package info
Run: `pip show aiohttp` - should show package info
  </verify>
  <done>aiohttp and aiolimiter added to requirements.txt and installed</done>
</task>

<task type="auto">
  <name>Task 2: Create ParallelAnalyzer service</name>
  <files>app/services/parallel_analyzer.py</files>
  <action>
Create `app/services/parallel_analyzer.py` implementing parallel batch analysis.

The module must include:

1. `ParallelAnalyzer` class:
   ```python
   class ParallelAnalyzer:
       def __init__(
           self,
           api_key: str,
           requests_per_minute: int = 100,  # Conservative for Tier 1
           max_concurrent: int = 5
       ):
           self.client = AsyncAnthropic(api_key=api_key)
           self.rate_limiter = AsyncLimiter(requests_per_minute, 60)
           self.semaphore = asyncio.Semaphore(max_concurrent)
           self.progress_lock = asyncio.Lock()
           self.progress = {'completed': 0, 'total': 0, 'risks_found': 0}
   ```

2. `analyze_batch()` method - analyze single batch with rate limiting:
   - Use `async with self.semaphore` for concurrent limit
   - Use `async with self.rate_limiter` for rate limiting
   - Call AsyncAnthropic messages.create()
   - Return dict with success/failure and response/error
   - Handle exceptions gracefully, return error dict instead of raising

3. `analyze_all()` method - analyze all batches in parallel:
   - Accept batches, build_prompts_fn, on_batch_complete callback
   - Create tasks for each batch
   - Use asyncio.gather(*tasks, return_exceptions=True)
   - Update progress atomically with asyncio.Lock
   - Call on_batch_complete callback after each batch

4. `run_parallel_analysis()` helper function:
   - Synchronous wrapper that runs async code via asyncio.run()
   - Creates ParallelAnalyzer instance
   - Accepts same parameters as analyze_document_with_llm needs
   - Returns list of results

Import structure:
```python
import asyncio
from typing import List, Dict, Any, Callable, Optional
from aiolimiter import AsyncLimiter
from anthropic import AsyncAnthropic
```

Key implementation notes:
- Model: "claude-opus-4-5-20251101" (same as current)
- Max tokens: 16000 (same as current)
- Handle both success and failure in results
- Count risks in successful responses for progress tracking
  </action>
  <verify>
Run: `python -c "from app.services.parallel_analyzer import ParallelAnalyzer, run_parallel_analysis; print('ParallelAnalyzer imported successfully')"`

Should print success message without import errors.
  </verify>
  <done>ParallelAnalyzer class exists with async batch processing, rate limiting via aiolimiter, and concurrent limit via semaphore</done>
</task>

<task type="auto">
  <name>Task 3: Integrate ParallelAnalyzer into claude_service</name>
  <files>app/services/claude_service.py</files>
  <action>
Modify `analyze_document_with_llm()` in `app/services/claude_service.py` to use ParallelAnalyzer for batch processing.

1. Add import at top:
   ```python
   from app.services.parallel_analyzer import ParallelAnalyzer
   import asyncio
   ```

2. Add `use_parallel: bool = True` parameter to function signature (default True, can be disabled for debugging)

3. Create a helper function for building prompts that can be passed to ParallelAnalyzer:
   ```python
   def build_batch_prompts(batch, contract_type, representation, aggressiveness, defined_terms, document_map):
       system_prompt = build_risk_analysis_prompt(contract_type, representation, aggressiveness)
       user_prompt = build_clause_batch_prompt(batch, defined_terms, document_map)
       return system_prompt, user_prompt
   ```

4. Replace the sequential batch processing loop (around lines 536-605) with parallel processing when use_parallel=True:

   ```python
   if use_parallel:
       # Parallel processing
       api_key = get_anthropic_api_key()
       analyzer = ParallelAnalyzer(api_key=api_key)

       # Create batches list
       batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]

       # Build prompts function
       def make_prompts(batch):
           return build_batch_prompts(batch, contract_type, representation, aggressiveness, defined_terms, document_map)

       # Progress callback
       async def on_progress(progress_data):
           if session_id:
               update_progress(session_id, {
                   'current_batch': progress_data['completed'],
                   'percent': int(progress_data['completed'] / progress_data['total'] * 100),
                   'risks_found': progress_data['risks_found']
               })

       # Run parallel analysis
       results = asyncio.run(analyzer.analyze_all(batches, make_prompts, on_progress))

       # Process results
       for i, result in enumerate(results):
           if isinstance(result, dict) and result.get('success'):
               batch_result = parse_risk_response(result['response'].content[0].text)
               all_risks.extend(batch_result.get('risks', []))
               # Merge concept map...
   else:
       # Keep existing sequential code as fallback
       for i in range(0, len(paragraphs), batch_size):
           # ... existing code ...
   ```

5. Keep all existing sequential code in an else branch for fallback/debugging

IMPORTANT: Do not remove any existing functionality. The sequential code should remain as a fallback that can be enabled by passing use_parallel=False.
  </action>
  <verify>
Run: `python -c "from app.services.claude_service import analyze_document_with_llm; print('claude_service with parallel support imported successfully')"`

Should print success message without import errors.
  </verify>
  <done>analyze_document_with_llm() uses ParallelAnalyzer by default, with sequential fallback available via use_parallel=False parameter</done>
</task>

</tasks>

<verification>
1. aiohttp and aiolimiter packages are installed
2. ParallelAnalyzer class can be imported without errors
3. claude_service.py imports and uses ParallelAnalyzer
4. Sequential fallback still works (use_parallel=False)
5. No regressions in risk parsing or concept map aggregation
</verification>

<success_criteria>
1. Multiple batches are processed concurrently (not sequentially)
2. Rate limiting prevents API errors (no 429s under normal load)
3. Max 5 concurrent API calls (semaphore enforced)
4. Progress tracking updates correctly during parallel execution
5. Failed batches don't crash entire analysis (graceful handling)
6. Logs show overlapping timestamps for concurrent batch processing
</success_criteria>

<output>
After completion, create `.planning/phases/06-analysis-acceleration/06-02-SUMMARY.md`
</output>
