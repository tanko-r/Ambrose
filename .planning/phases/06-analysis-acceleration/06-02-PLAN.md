---
phase: 06-analysis-acceleration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/initial_analyzer.py
  - app/services/claude_service.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Full document is sent to Claude Opus 4.5 in a single initial request"
    - "Initial analysis returns comprehensive concept map of entire document"
    - "All defined terms with definitions are extracted in initial pass"
    - "Cross-reference map identifies relationships between document sections"
    - "Conversation ID is captured for subsequent forking in parallel batches"
    - "Initial analysis result is stored in session for batch forks to inherit"
  artifacts:
    - path: "app/services/initial_analyzer.py"
      provides: "InitialDocumentAnalyzer class for full-document analysis"
      exports: ["InitialDocumentAnalyzer", "run_initial_analysis"]
    - path: "app/services/claude_service.py"
      provides: "Updated pipeline calling initial analysis before batches"
      contains: "initial_analysis"
  key_links:
    - from: "app/services/claude_service.py"
      to: "app/services/initial_analyzer.py"
      via: "import and delegation"
      pattern: "from app.services.initial_analyzer import"
    - from: "app/services/initial_analyzer.py"
      to: "anthropic.AsyncAnthropic"
      via: "API call with extended thinking"
      pattern: "AsyncAnthropic"
---

<objective>
Implement initial full-document analysis that sends the entire contract to Claude Opus 4.5 in one request, extracting a comprehensive concept map, defined terms, and cross-references that will be shared across all parallel batch forks.

Purpose: This is the foundation for the "conversation forking" architecture. By first analyzing the entire document holistically, we establish context that each parallel batch can inherit, eliminating the need for batches to "rediscover" document structure.

Output: InitialDocumentAnalyzer service that produces a conversation-ready context package.

PHASE 7 NOTE: This approach costs ~$6/document vs. ~$2 for the document-map approach. Phase 7 will implement a cheaper "document map" alternative for cost-conscious users. This plan includes infrastructure for toggling between approaches.
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-analysis-acceleration/06-RESEARCH.md

Key architecture notes:
- The initial analysis is the FIRST message in a conversation
- Response includes conversation_id (via response headers or first message ID)
- Batches will "fork" from this conversation, inheriting the full document context
- Anthropic SDK supports passing prior message history to continue conversations
- Extended thinking should be enabled for thorough document comprehension

Cost/speed tradeoff:
- This approach: ~$6/doc, ~90 seconds total (initial + parallel batches)
- Document map approach: ~$2/doc, ~15 minutes (sequential batches)
- 10x faster, 3x more expensive - Phase 7 will add toggle
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add async dependencies</name>
  <files>requirements.txt</files>
  <action>
Add the required async packages to requirements.txt:

```
aiohttp>=3.9.0
aiolimiter>=1.1.0
```

Note: The anthropic SDK already includes async support via AsyncAnthropic, so no additional install needed for that.

Run pip install to ensure dependencies are available:
```bash
pip install aiohttp aiolimiter
```
  </action>
  <verify>
Run: `pip show aiolimiter` - should show package info
Run: `pip show aiohttp` - should show package info
  </verify>
  <done>aiohttp and aiolimiter added to requirements.txt and installed</done>
</task>

<task type="auto">
  <name>Task 2: Create InitialDocumentAnalyzer service</name>
  <files>app/services/initial_analyzer.py</files>
  <action>
Create `app/services/initial_analyzer.py` implementing the initial full-document analysis.

The module must include:

1. `InitialDocumentAnalyzer` class:
   ```python
   class InitialDocumentAnalyzer:
       """
       Performs initial full-document analysis to establish context for batch forking.

       This sends the ENTIRE document to Claude Opus 4.5 in one request, extracting:
       - Comprehensive concept map (document structure)
       - Defined terms with their definitions
       - Cross-reference map (which sections reference which)
       - Document type and key characteristics

       The conversation ID from this request is used to "fork" parallel batch
       analyses that inherit this full-document understanding.
       """

       def __init__(self, api_key: str):
           self.client = anthropic.Anthropic(api_key=api_key)
           self.async_client = AsyncAnthropic(api_key=api_key)
   ```

2. `build_initial_analysis_prompt()` method - creates system + user prompt for full doc:
   ```python
   def build_initial_analysis_prompt(self, document_text: str, contract_type: str, representation: str) -> tuple[str, str]:
       """Build prompts for initial full-document analysis."""
       system_prompt = f"""You are a senior attorney performing an initial review of a contract.

       Your task is to create a comprehensive analysis of this document's structure that will guide detailed paragraph-by-paragraph review.

       Extract and return:
       1. CONCEPT_MAP: Hierarchical structure of the document (sections, subsections, their purposes)
       2. DEFINED_TERMS: All defined terms with their exact definitions
       3. CROSS_REFERENCES: Which sections reference other sections
       4. DOCUMENT_PROFILE: Contract type, parties, key dates, critical provisions

       Representation: {representation}
       Contract Type: {contract_type}

       Return as structured JSON with these exact keys:
       - concept_map: nested object of sections
       - defined_terms: array of {{term, definition, location}}
       - cross_references: array of {{from_section, to_section, reference_type}}
       - document_profile: object with type, parties, dates, key_provisions
       """

       user_prompt = f"""Analyze this complete contract document:

{document_text}

Return the structured analysis as JSON."""

       return system_prompt, user_prompt
   ```

3. `analyze()` async method - performs the initial analysis:
   ```python
   async def analyze(
       self,
       paragraphs: List[Dict],
       contract_type: str,
       representation: str,
       on_progress: Optional[Callable] = None
   ) -> Dict:
       """
       Perform initial full-document analysis.

       Returns:
           {
               'concept_map': {...},
               'defined_terms': [...],
               'cross_references': [...],
               'document_profile': {...},
               'conversation_messages': [...],  # For forking
               'initial_response_id': str,       # Message ID for reference
           }
       """
       # Build full document text
       document_text = "\n\n".join([
           f"[{p.get('id', 'unknown')}] {p.get('text', '')}"
           for p in paragraphs
       ])

       system_prompt, user_prompt = self.build_initial_analysis_prompt(
           document_text, contract_type, representation
       )

       if on_progress:
           await on_progress({'stage': 'initial_analysis', 'status': 'sending_full_document'})

       # Use extended thinking for thorough comprehension
       response = await self.async_client.messages.create(
           model="claude-opus-4-5-20251101",
           max_tokens=32000,
           thinking={
               "type": "enabled",
               "budget_tokens": 10000  # Allow thinking for complex doc analysis
           },
           system=system_prompt,
           messages=[{"role": "user", "content": user_prompt}]
       )

       if on_progress:
           await on_progress({'stage': 'initial_analysis', 'status': 'parsing_response'})

       # Parse response
       result = self._parse_initial_response(response)

       # Store conversation context for forking
       result['conversation_messages'] = [
           {"role": "user", "content": user_prompt},
           {"role": "assistant", "content": response.content[0].text}
       ]
       result['initial_response_id'] = response.id
       result['system_prompt'] = system_prompt

       return result
   ```

4. `_parse_initial_response()` helper - extracts JSON from response:
   ```python
   def _parse_initial_response(self, response) -> Dict:
       """Parse the initial analysis response."""
       # Handle thinking blocks
       text_content = ""
       for block in response.content:
           if hasattr(block, 'text'):
               text_content = block.text
               break

       # Try to extract JSON
       try:
           # Look for JSON in code blocks
           import re
           json_match = re.search(r'```json\s*(.*?)\s*```', text_content, re.DOTALL)
           if json_match:
               return json.loads(json_match.group(1))
           # Try direct parse
           return json.loads(text_content)
       except json.JSONDecodeError:
           # Fallback: return raw text for manual parsing
           return {
               'concept_map': {},
               'defined_terms': [],
               'cross_references': [],
               'document_profile': {},
               'raw_response': text_content
           }
   ```

5. `run_initial_analysis()` synchronous wrapper:
   ```python
   def run_initial_analysis(
       api_key: str,
       paragraphs: List[Dict],
       contract_type: str,
       representation: str
   ) -> Dict:
       """Synchronous wrapper for initial analysis."""
       analyzer = InitialDocumentAnalyzer(api_key)
       return asyncio.run(analyzer.analyze(paragraphs, contract_type, representation))
   ```

Import structure:
```python
import asyncio
import json
import re
from typing import List, Dict, Any, Optional, Callable
import anthropic
from anthropic import AsyncAnthropic
```
  </action>
  <verify>
Run: `python -c "from app.services.initial_analyzer import InitialDocumentAnalyzer, run_initial_analysis; print('InitialDocumentAnalyzer imported successfully')"`

Should print success message without import errors.
  </verify>
  <done>InitialDocumentAnalyzer class exists with full-document analysis, returning concept map, defined terms, cross-references, and conversation context for forking</done>
</task>

<task type="auto">
  <name>Task 3: Integrate initial analysis into claude_service pipeline</name>
  <files>app/services/claude_service.py</files>
  <action>
Modify `analyze_document_with_llm()` in `app/services/claude_service.py` to call initial analysis before batch processing.

1. Add import at top:
   ```python
   from app.services.initial_analyzer import run_initial_analysis
   ```

2. Add `use_forking: bool = True` parameter to function signature (enables new architecture)

3. After content filtering but BEFORE batch loop, add initial analysis call:
   ```python
   initial_context = None
   if use_forking:
       # Perform initial full-document analysis
       if session_id:
           update_progress(session_id, {
               'status': 'analyzing',
               'current_action': 'Performing initial document analysis (this establishes context for parallel batch processing)...',
               'stage': 'initial_analysis',
               'percent': 5
           })

       try:
           api_key = get_anthropic_api_key()
           initial_context = run_initial_analysis(
               api_key=api_key,
               paragraphs=paragraphs,
               contract_type=contract_type,
               representation=representation
           )

           # Use extracted defined terms and concept map
           defined_terms = [t['term'] for t in initial_context.get('defined_terms', [])]
           document_map = json.dumps(initial_context.get('concept_map', {}), indent=2)

           if session_id:
               update_progress(session_id, {
                   'initial_analysis_complete': True,
                   'defined_terms_count': len(defined_terms),
                   'current_action': 'Initial analysis complete. Starting parallel batch analysis...',
                   'percent': 15
               })

       except Exception as e:
           print(f"Initial analysis failed, falling back to sequential: {e}")
           initial_context = None
           if session_id:
               update_progress(session_id, {
                   'initial_analysis_error': str(e),
                   'current_action': 'Initial analysis failed, using sequential analysis...'
               })
   ```

4. Store initial_context in session for batch forks (will be used in Plan 03):
   ```python
   if initial_context and session_id:
       # Store conversation context for parallel batch forking
       # This will be used by ParallelAnalyzer in Plan 03
       update_progress(session_id, {
           'initial_context': initial_context
       })
   ```

5. Include initial_context metadata in final result:
   ```python
   # In the return dict:
   'summary': {
       ...
       'used_forking': use_forking and initial_context is not None,
       'initial_defined_terms': len(defined_terms) if initial_context else 0,
       ...
   }
   ```

IMPORTANT: Keep ALL existing batch processing code intact. The initial_context will be used by Plan 03 to fork conversations. For now, the batch loop still runs sequentially but uses the richer context from initial analysis.
  </action>
  <verify>
Run: `python -c "from app.services.claude_service import analyze_document_with_llm; print('claude_service with initial analysis imported successfully')"`

Should print success message without import errors.
  </verify>
  <done>analyze_document_with_llm() calls initial analysis first, extracts defined terms and concept map, stores conversation context for batch forking</done>
</task>

</tasks>

<verification>
1. aiohttp and aiolimiter packages are installed
2. InitialDocumentAnalyzer class can be imported without errors
3. claude_service.py imports and calls initial analysis
4. Initial analysis returns concept_map, defined_terms, cross_references
5. Conversation messages are stored for forking in Plan 03
</verification>

<success_criteria>
1. Full document is sent to Claude Opus 4.5 in initial request
2. Response includes comprehensive concept map
3. All defined terms extracted with definitions
4. Cross-reference map identifies section relationships
5. Conversation context stored for batch forking
6. Progress shows "Performing initial document analysis..." stage
7. Graceful fallback if initial analysis fails
</success_criteria>

<output>
After completion, create `.planning/phases/06-analysis-acceleration/06-02-SUMMARY.md`
</output>
