---
phase: 06-analysis-acceleration
plan: 03
type: execute
wave: 2
depends_on:
  - "06-01"
  - "06-02"
files_modified:
  - app/services/parallel_analyzer.py
  - app/services/claude_service.py
autonomous: true

must_haves:
  truths:
    - "Each batch analysis forks from the initial conversation, inheriting full document context"
    - "Up to 30 parallel forks execute simultaneously, each analyzing 5 paragraphs"
    - "Rate limiting prevents 429 errors from Anthropic API"
    - "Failed batches are handled gracefully without crashing entire analysis"
    - "Batch results are aggregated into unified risk list"
    - "Total analysis time for 150-paragraph document is under 2 minutes"
  artifacts:
    - path: "app/services/parallel_analyzer.py"
      provides: "ForkedParallelAnalyzer class for conversation-forking batch analysis"
      exports: ["ForkedParallelAnalyzer", "run_forked_parallel_analysis"]
    - path: "app/services/claude_service.py"
      provides: "Updated pipeline using forked parallel batches"
      contains: "ForkedParallelAnalyzer"
  key_links:
    - from: "app/services/claude_service.py"
      to: "app/services/parallel_analyzer.py"
      via: "import and delegation"
      pattern: "from app.services.parallel_analyzer import"
    - from: "app/services/parallel_analyzer.py"
      to: "initial_context.conversation_messages"
      via: "message history inheritance"
      pattern: "conversation_messages"
    - from: "app/services/parallel_analyzer.py"
      to: "anthropic.AsyncAnthropic"
      via: "async client with forked conversations"
      pattern: "AsyncAnthropic"
---

<objective>
Implement forked parallel batch analysis where each batch inherits the full document context from the initial analysis conversation, enabling true parallelism with shared understanding.

Purpose: The key insight is that LLM conversations can be "forked" - each parallel batch starts with the same initial message history (the full document analysis from Plan 02), then adds its specific batch prompt. This means every batch has full document context without needing to resend the document.

Output: ForkedParallelAnalyzer that runs 30 parallel batches, each inheriting initial document context.

PHASE 7 NOTE: This approach costs ~$6/document (initial + 30 parallel) vs. ~$2 for sequential document-map approach (no initial, sequential batches). Phase 7 will implement `analysis_mode: 'fast' | 'economical'` toggle. The current implementation is the "fast" path.
</objective>

<execution_context>
@C:\Users\david\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\david\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-analysis-acceleration/06-RESEARCH.md
@.planning/phases/06-analysis-acceleration/06-02-SUMMARY.md (if exists)

Key architecture notes:
- Plan 02 produces `initial_context` with:
  - `conversation_messages`: [user_msg, assistant_msg] from initial analysis
  - `system_prompt`: The system prompt used
  - `concept_map`, `defined_terms`, etc.

- Each forked batch call includes:
  - Same `system` prompt
  - Same initial messages (full document + initial analysis response)
  - NEW message: "Now analyze these specific paragraphs: [batch]"

- This is NOT conversation continuation (sequential)
- This IS conversation forking (parallel) - each fork is independent

Cost breakdown (~$6 total):
- Initial analysis: ~$2 (full doc with thinking)
- 30 parallel batches: ~$0.13 each = ~$4 (inherit context, no thinking needed)

Speed breakdown (~90 seconds):
- Initial analysis: ~45 seconds (thinking enabled)
- 30 parallel batches: ~30-45 seconds (run simultaneously)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ForkedParallelAnalyzer service</name>
  <files>app/services/parallel_analyzer.py</files>
  <action>
Create `app/services/parallel_analyzer.py` implementing forked parallel batch analysis.

The module must include:

1. `ForkedParallelAnalyzer` class:
   ```python
   class ForkedParallelAnalyzer:
       """
       Performs parallel batch analysis by forking from an initial conversation.

       Architecture:
       - Initial analysis (Plan 02) creates first conversation with full document
       - This class creates N parallel "forks" of that conversation
       - Each fork inherits full document context but analyzes specific paragraphs
       - Results are aggregated into unified risk list

       Cost/Speed tradeoff:
       - ~$6/document, ~90 seconds total
       - Phase 7 will add 'economical' mode: ~$2/doc, ~15 minutes
       """

       def __init__(
           self,
           api_key: str,
           requests_per_minute: int = 150,  # Higher limit for Opus
           max_concurrent: int = 30         # 30 parallel forks
       ):
           self.client = AsyncAnthropic(api_key=api_key)
           self.rate_limiter = AsyncLimiter(requests_per_minute, 60)
           self.semaphore = asyncio.Semaphore(max_concurrent)
           self.progress_lock = asyncio.Lock()
           self.progress = {'completed': 0, 'total': 0, 'risks_found': 0}
   ```

2. `build_batch_fork_prompt()` method - creates the fork message for a batch:
   ```python
   def build_batch_fork_prompt(
       self,
       batch: List[Dict],
       batch_num: int,
       total_batches: int
   ) -> str:
       """Build the fork prompt for analyzing a specific batch of paragraphs."""
       paragraphs_text = "\n\n".join([
           f"[{p.get('id', f'para_{i}')}] {p.get('text', '')}"
           for i, p in enumerate(batch)
       ])

       return f"""Based on your comprehensive understanding of this document from the initial analysis, now perform detailed risk/opportunity analysis on these specific paragraphs (batch {batch_num} of {total_batches}):

{paragraphs_text}

For each paragraph, identify:
1. RISKS: Provisions that could harm our client's interests
2. OPPORTUNITIES: Ways to strengthen our client's position
3. CROSS_REFERENCES: How this paragraph relates to other document sections you analyzed

Return as JSON:
{{
  "risks": [
    {{
      "risk_id": "unique_id",
      "para_id": "paragraph_id",
      "severity": "high|medium|low",
      "category": "liability|termination|timing|etc",
      "title": "brief_title",
      "description": "detailed explanation",
      "affected_text": "the specific problematic language",
      "recommendation": "suggested revision or action"
    }}
  ],
  "opportunities": [...]
}}"""
   ```

3. `analyze_batch_fork()` async method - executes one forked batch:
   ```python
   async def analyze_batch_fork(
       self,
       batch: List[Dict],
       batch_num: int,
       total_batches: int,
       initial_context: Dict
   ) -> Dict[str, Any]:
       """
       Analyze a batch by forking from the initial conversation.

       The fork includes:
       - Same system prompt as initial analysis
       - Initial user message (full document)
       - Initial assistant response (concept map, etc.)
       - NEW: Batch-specific analysis request
       """
       async with self.semaphore:
           async with self.rate_limiter:
               try:
                   # Build forked message history
                   messages = initial_context['conversation_messages'].copy()

                   # Add batch-specific prompt
                   batch_prompt = self.build_batch_fork_prompt(batch, batch_num, total_batches)
                   messages.append({"role": "user", "content": batch_prompt})

                   # Call API with forked conversation
                   response = await self.client.messages.create(
                       model="claude-opus-4-5-20251101",
                       max_tokens=8000,
                       system=initial_context.get('system_prompt', ''),
                       messages=messages
                   )

                   return {
                       'success': True,
                       'batch_num': batch_num,
                       'response': response,
                       'paragraph_ids': [p.get('id') for p in batch]
                   }

               except Exception as e:
                   return {
                       'success': False,
                       'batch_num': batch_num,
                       'error': str(e),
                       'paragraph_ids': [p.get('id') for p in batch]
                   }
   ```

4. `analyze_all_batches()` async method - orchestrates all parallel forks:
   ```python
   async def analyze_all_batches(
       self,
       batches: List[List[Dict]],
       initial_context: Dict,
       on_batch_complete: Optional[Callable] = None
   ) -> List[Dict]:
       """
       Analyze all batches in parallel via conversation forking.

       Args:
           batches: List of paragraph batches (each batch is ~5 paragraphs)
           initial_context: From Plan 02 initial analysis
           on_batch_complete: Optional callback for progress updates
       """
       self.progress['total'] = len(batches)
       self.progress['completed'] = 0
       self.progress['risks_found'] = 0

       async def process_batch(batch_idx: int, batch: List[Dict]):
           result = await self.analyze_batch_fork(
               batch, batch_idx + 1, len(batches), initial_context
           )

           async with self.progress_lock:
               self.progress['completed'] += 1
               if result['success']:
                   # Parse and count risks
                   risks = self._parse_batch_response(result['response'])
                   result['risks'] = risks
                   self.progress['risks_found'] += len(risks)

               if on_batch_complete:
                   await on_batch_complete(self.progress.copy(), result)

           return result

       # Create all tasks
       tasks = [
           process_batch(i, batch)
           for i, batch in enumerate(batches)
       ]

       # Run all in parallel
       results = await asyncio.gather(*tasks, return_exceptions=True)

       # Handle any exceptions that weren't caught
       processed_results = []
       for i, result in enumerate(results):
           if isinstance(result, Exception):
               processed_results.append({
                   'success': False,
                   'batch_num': i + 1,
                   'error': str(result)
               })
           else:
               processed_results.append(result)

       return processed_results
   ```

5. `_parse_batch_response()` helper - extracts risks from response:
   ```python
   def _parse_batch_response(self, response) -> List[Dict]:
       """Parse risks from a batch response."""
       try:
           text = response.content[0].text

           # Try to extract JSON
           import re
           json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
           if json_match:
               data = json.loads(json_match.group(1))
           else:
               data = json.loads(text)

           risks = data.get('risks', [])
           opportunities = data.get('opportunities', [])

           # Combine risks and opportunities (opportunities are also "risks" in our model)
           return risks + opportunities

       except (json.JSONDecodeError, AttributeError, IndexError) as e:
           print(f"Failed to parse batch response: {e}")
           return []
   ```

6. `run_forked_parallel_analysis()` synchronous wrapper:
   ```python
   def run_forked_parallel_analysis(
       api_key: str,
       paragraphs: List[Dict],
       initial_context: Dict,
       batch_size: int = 5,
       on_progress: Optional[Callable] = None
   ) -> Dict:
       """
       Synchronous wrapper for forked parallel analysis.

       Returns:
           {
               'risks': [...],
               'batch_results': [...],
               'stats': {
                   'total_batches': int,
                   'successful_batches': int,
                   'failed_batches': int,
                   'total_risks': int
               }
           }
       """
       # Create batches
       batches = [
           paragraphs[i:i + batch_size]
           for i in range(0, len(paragraphs), batch_size)
       ]

       analyzer = ForkedParallelAnalyzer(api_key)

       # Run async
       results = asyncio.run(
           analyzer.analyze_all_batches(batches, initial_context, on_progress)
       )

       # Aggregate results
       all_risks = []
       successful = 0
       failed = 0

       for result in results:
           if result.get('success'):
               all_risks.extend(result.get('risks', []))
               successful += 1
           else:
               failed += 1

       return {
           'risks': all_risks,
           'batch_results': results,
           'stats': {
               'total_batches': len(batches),
               'successful_batches': successful,
               'failed_batches': failed,
               'total_risks': len(all_risks)
           }
       }
   ```

Import structure:
```python
import asyncio
import json
import re
from typing import List, Dict, Any, Optional, Callable
from aiolimiter import AsyncLimiter
from anthropic import AsyncAnthropic
```
  </action>
  <verify>
Run: `python -c "from app.services.parallel_analyzer import ForkedParallelAnalyzer, run_forked_parallel_analysis; print('ForkedParallelAnalyzer imported successfully')"`

Should print success message without import errors.
  </verify>
  <done>ForkedParallelAnalyzer class exists with conversation forking, 30 parallel batches, rate limiting, and result aggregation</done>
</task>

<task type="auto">
  <name>Task 2: Integrate forked parallel analysis into claude_service</name>
  <files>app/services/claude_service.py</files>
  <action>
Modify `analyze_document_with_llm()` in `app/services/claude_service.py` to use forked parallel analysis when initial_context is available.

1. Add import at top:
   ```python
   from app.services.parallel_analyzer import run_forked_parallel_analysis
   ```

2. Replace the existing sequential batch loop with conditional forked/sequential logic:
   ```python
   # After initial analysis (from Plan 02) and before the batch loop

   if use_forking and initial_context:
       # ===== FORKED PARALLEL PATH (fast, ~$6/doc, ~90 seconds) =====
       if session_id:
           update_progress(session_id, {
               'current_action': f'Running {total_batches} parallel batch analyses (forked from initial context)...',
               'stage': 'parallel_batches',
               'percent': 20
           })

       def progress_callback(progress_data, batch_result=None):
           if session_id:
               completed = progress_data['completed']
               total = progress_data['total']
               pct = 20 + int(completed / total * 75)  # 20-95%

               update_progress(session_id, {
                   'current_batch': completed,
                   'total_batches': total,
                   'risks_found': progress_data['risks_found'],
                   'percent': pct,
                   'current_action': f'Completed batch {completed}/{total} ({progress_data["risks_found"]} risks found so far)'
               })

       # Run forked parallel analysis
       api_key = get_anthropic_api_key()
       parallel_result = run_forked_parallel_analysis(
           api_key=api_key,
           paragraphs=paragraphs,
           initial_context=initial_context,
           batch_size=batch_size,
           on_progress=progress_callback
       )

       all_risks = parallel_result['risks']

       # Store batch stats for summary
       batch_stats = parallel_result['stats']

   else:
       # ===== SEQUENTIAL PATH (economical, ~$2/doc, ~15 minutes) =====
       # Keep existing sequential batch processing code exactly as is
       # This becomes the fallback when use_forking=False or initial_context failed

       for i in range(0, len(paragraphs), batch_size):
           # ... existing sequential code ...
   ```

3. Update the final summary to include forking stats:
   ```python
   'summary': {
       ...
       'analysis_mode': 'forked_parallel' if (use_forking and initial_context) else 'sequential',
       'parallel_stats': batch_stats if (use_forking and initial_context) else None,
       'estimated_cost': '$6' if (use_forking and initial_context) else '$2',
       ...
   }
   ```

4. Add comment for Phase 7 toggle:
   ```python
   # TODO (Phase 7): Add analysis_mode parameter to toggle between:
   # - 'fast': use_forking=True, ~$6/doc, ~90 seconds
   # - 'economical': use_forking=False, ~$2/doc, ~15 minutes
   # Expose this choice in intake form
   ```

IMPORTANT: Keep ALL existing sequential batch code as the else branch. The sequential path remains as:
- Fallback when forking fails
- Future "economical" mode for Phase 7
  </action>
  <verify>
Run: `python -c "from app.services.claude_service import analyze_document_with_llm; print('claude_service with forked parallel analysis imported successfully')"`

Should print success message without import errors.
  </verify>
  <done>analyze_document_with_llm() uses forked parallel analysis when initial_context available, with sequential fallback</done>
</task>

</tasks>

<verification>
1. ForkedParallelAnalyzer class can be imported without errors
2. claude_service.py imports and uses ForkedParallelAnalyzer
3. Forked batches inherit conversation_messages from initial_context
4. Rate limiting prevents 429 errors (150 RPM limit)
5. Max 30 concurrent batch forks
6. Sequential fallback still works when use_forking=False
7. Risks are properly aggregated from all batch results
</verification>

<success_criteria>
1. Each batch fork includes initial conversation messages (inherits full doc context)
2. 30 parallel batches execute simultaneously
3. Total analysis time for 150-paragraph document < 2 minutes
4. Rate limiting prevents API errors under parallel load
5. Failed batches logged but don't crash analysis
6. Progress shows batch completion count during parallel execution
7. Summary includes analysis_mode and cost estimate
</success_criteria>

<output>
After completion, create `.planning/phases/06-analysis-acceleration/06-03-SUMMARY.md`
</output>
