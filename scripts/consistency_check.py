#!/usr/bin/env python3
"""
Phase 4: Consistency Check, Deferred Modifications & Assembly

Processes deferred modifications, verifies internal consistency,
and assembles the final document.

Steps:
4A: Process deferred modifications
4B: Consistency verification
4C: Final corrections
4D: Assembly and output generation
"""

import json
import sys
import re
from pathlib import Path
from datetime import datetime
from copy import deepcopy


def load_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_json(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


class DeferredModificationProcessor:
    """
    Processes deferred modifications from Phase 3.
    """

    def __init__(self, revised_content, deferred_modifications, analysis):
        self.content = revised_content
        self.modifications = deferred_modifications
        self.analysis = analysis
        self.applied = []

    def process_all(self):
        """Process all deferred modifications."""
        for mod in self.modifications:
            result = self._apply_modification(mod)
            self.applied.append({
                'modification': mod,
                'result': result
            })

        return self.content, self.applied

    def _apply_modification(self, mod):
        """Apply a single deferred modification."""
        mod_type = mod.get('type', '')
        target = mod.get('target_location', '')
        description = mod.get('description', '')

        if mod_type == 'add_definition':
            return self._add_definition(target, description)
        elif mod_type == 'conforming_change':
            return self._apply_conforming_change(target, description)
        elif mod_type == 'cross_reference':
            return self._update_cross_reference(target, description)
        else:
            return {'status': 'skipped', 'reason': f'Unknown modification type: {mod_type}'}

    def _add_definition(self, target, description):
        """Add a new definition to the definitions section."""
        # Find definitions section
        definitions_idx = None
        for i, item in enumerate(self.content):
            if item.get('type') != 'paragraph':
                continue
            text = item.get('text', '').lower()
            caption = (item.get('caption', '') or '').lower()
            if 'definition' in caption or ('"' in text and 'means' in text):
                definitions_idx = i
                break

        if definitions_idx is None:
            return {'status': 'failed', 'reason': 'Could not find definitions section'}

        # The actual definition text should be provided by LLM
        return {
            'status': 'pending_llm',
            'reason': 'Definition text needs to be generated by LLM',
            'target_index': definitions_idx,
            'description': description
        }

    def _apply_conforming_change(self, target, description):
        """Apply a conforming change to another section."""
        # Find the target section
        for i, item in enumerate(self.content):
            if item.get('type') != 'paragraph':
                continue
            section_ref = item.get('section_ref', '')
            if target.lower() in section_ref.lower():
                return {
                    'status': 'pending_llm',
                    'reason': 'Conforming change needs LLM review',
                    'target_index': i,
                    'target_section': section_ref,
                    'description': description
                }

        return {'status': 'failed', 'reason': f'Could not find target section: {target}'}

    def _update_cross_reference(self, target, description):
        """Update a cross-reference."""
        return {
            'status': 'pending_llm',
            'reason': 'Cross-reference update needs LLM review',
            'target': target,
            'description': description
        }


class ConsistencyChecker:
    """
    Verifies internal consistency of the revised document.
    """

    def __init__(self, revised_content, analysis):
        self.content = revised_content
        self.analysis = analysis
        self.issues = []

    def check_all(self):
        """Run all consistency checks."""
        self._check_duplicate_concepts()
        self._check_cross_references()
        self._check_defined_terms()
        self._check_conflicting_provisions()
        self._check_orphaned_content()
        self._check_logical_flow()

        return self.issues

    def _check_duplicate_concepts(self):
        """Check for concepts that appear multiple times."""
        # Track deliverables, definitions, etc.
        deliverables = []
        definitions = {}

        for item in self.content:
            if item.get('type') != 'paragraph':
                continue

            text = item.get('text', '')
            para_id = item.get('id', '')

            # Check for deliverables (shall deliver X)
            deliverable_match = re.findall(r'shall deliver[^.]*?(?:the\s+)?([A-Z][^,.;]+)', text)
            for d in deliverable_match:
                d_lower = d.lower().strip()
                for prev_d, prev_id in deliverables:
                    if d_lower == prev_d or (len(d_lower) > 10 and d_lower in prev_d) or (len(prev_d) > 10 and prev_d in d_lower):
                        self.issues.append({
                            'type': 'duplicate_deliverable',
                            'severity': 'warning',
                            'description': f'Possible duplicate deliverable: "{d}"',
                            'locations': [prev_id, para_id]
                        })
                deliverables.append((d_lower, para_id))

            # Check for duplicate definitions
            def_match = re.findall(r'"([A-Z][^"]+)"\s+(?:means|shall mean)', text)
            for term in def_match:
                if term in definitions:
                    self.issues.append({
                        'type': 'duplicate_definition',
                        'severity': 'error',
                        'description': f'Term "{term}" is defined multiple times',
                        'locations': [definitions[term], para_id]
                    })
                else:
                    definitions[term] = para_id

    def _check_cross_references(self):
        """Check for broken cross-references."""
        # Collect all section numbers
        section_numbers = set()
        for item in self.content:
            if item.get('type') != 'paragraph':
                continue
            hierarchy = item.get('section_hierarchy', [])
            for h in hierarchy:
                num = h.get('number', '').rstrip('.')
                if num:
                    section_numbers.add(num)

        # Check references
        for item in self.content:
            if item.get('type') != 'paragraph':
                continue
            text = item.get('text', '')
            para_id = item.get('id', '')

            # Find section references
            refs = re.findall(r'Section\s+(\d+(?:\.\d+)*)', text, re.IGNORECASE)
            for ref in refs:
                # Check if this section exists
                ref_clean = ref.rstrip('.')
                if ref_clean not in section_numbers:
                    # Might be subsection
                    parts = ref_clean.split('.')
                    if parts[0] not in section_numbers:
                        self.issues.append({
                            'type': 'broken_reference',
                            'severity': 'warning',
                            'description': f'Reference to Section {ref} may be broken',
                            'location': para_id
                        })

    def _check_defined_terms(self):
        """Check for inconsistent defined term usage."""
        # Get all defined terms from analysis
        defined_terms = {}
        for term_data in self.analysis['defined_terms'].get('target', []):
            term = term_data.get('term', '')
            defined_terms[term.lower()] = term

        # Check usage
        for item in self.content:
            if item.get('type') != 'paragraph':
                continue
            text = item.get('text', '')
            para_id = item.get('id', '')

            # Look for variations of defined terms
            for term_lower, term_proper in defined_terms.items():
                # Check if term is used with different capitalization
                pattern = re.compile(re.escape(term_proper), re.IGNORECASE)
                matches = pattern.findall(text)
                for match in matches:
                    if match != term_proper and not text.startswith(match):
                        self.issues.append({
                            'type': 'term_inconsistency',
                            'severity': 'info',
                            'description': f'Term "{match}" should be "{term_proper}"',
                            'location': para_id
                        })

    def _check_conflicting_provisions(self):
        """Check for potentially conflicting provisions."""
        # Track survival periods
        survival_periods = []

        for item in self.content:
            if item.get('type') != 'paragraph':
                continue
            text = item.get('text', '').lower()
            para_id = item.get('id', '')

            # Check for survival periods
            survival_match = re.search(r'survive.*?(\d+)\s*(?:months?|years?|days?)', text)
            if survival_match:
                period = survival_match.group(0)
                survival_periods.append((period, para_id))

        # Check for conflicting survival periods
        if len(survival_periods) > 1:
            periods_text = [p[0] for p in survival_periods]
            if len(set(periods_text)) > 1:
                self.issues.append({
                    'type': 'conflicting_survival',
                    'severity': 'warning',
                    'description': f'Multiple different survival periods found: {periods_text}',
                    'locations': [p[1] for p in survival_periods]
                })

    def _check_orphaned_content(self):
        """Check for orphaned insertions or disconnected content."""
        for item in self.content:
            if item.get('type') != 'paragraph':
                continue

            if item.get('is_insertion'):
                text = item.get('text', '')
                para_id = item.get('id', '')

                # Check if inserted content references terms that don't exist
                # (This is a simplified check - could be more sophisticated)
                if 'as defined' in text.lower() or 'pursuant to' in text.lower():
                    self.issues.append({
                        'type': 'orphaned_insertion',
                        'severity': 'info',
                        'description': 'Inserted content may reference undefined terms',
                        'location': para_id
                    })

    def _check_logical_flow(self):
        """Check for logical flow issues."""
        # Look for references to caps/baskets that might not exist
        has_cap = False
        cap_references = []

        for item in self.content:
            if item.get('type') != 'paragraph':
                continue
            text = item.get('text', '').lower()
            para_id = item.get('id', '')

            # Check for cap definitions
            if 'liability cap' in text or 'shall not exceed' in text or 'aggregate' in text and '$' in text:
                has_cap = True

            # Check for cap references
            if 'subject to the' in text and 'cap' in text:
                cap_references.append(para_id)

        if cap_references and not has_cap:
            self.issues.append({
                'type': 'missing_cap',
                'severity': 'error',
                'description': 'References to liability cap found but no cap is defined',
                'locations': cap_references
            })


def generate_manifest(revised_content, changes_log, context, output_path):
    """Generate the hierarchical change manifest."""
    lines = [
        "# Contract Redline Manifest",
        "",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "## Context",
        f"- **Representation**: {context.get('representation', 'Seller').title()}",
        f"- **Total Changes**: {len(changes_log)}",
        "",
        "---",
        "",
        "## Changes by Section",
        ""
    ]

    current_section = None

    for change in changes_log:
        section = change.get('section_name', change.get('section_id', 'Unknown'))
        if section != current_section:
            current_section = section
            lines.append(f"### {section}")
            lines.append("")

        para_id = change.get('paragraph_id', '')
        rationale = change.get('rationale', 'No rationale provided')
        changes_made = change.get('changes_made', [])

        if changes_made:
            changes_str = '; '.join(changes_made[:3])
            if len(changes_made) > 3:
                changes_str += f' (+{len(changes_made) - 3} more)'
        else:
            changes_str = 'Modified'

        lines.append(f"- **{para_id}**: {changes_str}")
        lines.append(f"  - *Rationale*: {rationale}")
        lines.append("")

    # Add insertions section
    insertions = [c for c in changes_log if c.get('type') == 'insertion']
    if insertions:
        lines.append("---")
        lines.append("")
        lines.append("## New Sections Inserted")
        lines.append("")
        for ins in insertions:
            lines.append(f"- **{ins.get('concept', 'New Section')}**: {ins.get('rationale', '')}")
        lines.append("")

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))


def build_revised_json(original_parsed, revised_content, context):
    """Build the final revised.json structure."""
    return {
        'source_file': original_parsed.get('source_file', ''),
        'revision_date': datetime.now().strftime('%Y-%m-%d %H:%M'),
        'context': context,
        'content': revised_content,
        'defined_terms': original_parsed.get('defined_terms', []),
        'sections': original_parsed.get('sections', []),
        'exhibits': original_parsed.get('exhibits', [])
    }


def assemble_revised_content(original_parsed, section_results):
    """
    Assemble revised content from section results.

    Takes the original document and applies revisions from each section.
    """
    revised_content = deepcopy(original_parsed.get('content', []))

    # Build lookup from para_id to index
    para_index = {}
    for i, item in enumerate(revised_content):
        if item.get('type') == 'paragraph':
            para_index[item.get('id')] = i
        elif item.get('type') == 'table':
            # Also index table cell paragraphs
            for row in item.get('rows', []):
                for cell in row:
                    for para in cell.get('paragraphs', []):
                        para_index[para.get('id')] = ('table', i, para.get('id'))

    # Apply revisions from each section
    changes_log = []

    for section_result in section_results:
        section_id = section_result.get('section_id', '')
        section_name = section_result.get('section_name', section_id)

        for para_revision in section_result.get('revised_paragraphs', []):
            para_id = para_revision.get('paragraph_id')
            status = para_revision.get('status', 'no_change')

            if status == 'no_change':
                continue

            if para_id in para_index:
                idx = para_index[para_id]

                if isinstance(idx, tuple):
                    # Table cell paragraph
                    _, table_idx, cell_para_id = idx
                    # Would need to navigate table structure
                    continue
                else:
                    # Regular paragraph
                    revised_content[idx]['text'] = para_revision.get('revised_text', '')
                    revised_content[idx]['rationale'] = para_revision.get('rationale', '')
                    revised_content[idx]['changes_made'] = para_revision.get('changes_made', [])

                    changes_log.append({
                        'section_id': section_id,
                        'section_name': section_name,
                        'paragraph_id': para_id,
                        'rationale': para_revision.get('rationale', ''),
                        'changes_made': para_revision.get('changes_made', [])
                    })

    return revised_content, changes_log


def main():
    """
    Main entry point for consistency check and assembly.

    Usage:
        python consistency_check.py <original_parsed.json> <section_results_dir> <output_dir>
    """
    if len(sys.argv) < 4:
        print("Usage: python consistency_check.py <original_parsed.json> <section_results_dir> <output_dir>")
        sys.exit(1)

    original_path = Path(sys.argv[1])
    results_dir = Path(sys.argv[2])
    output_dir = Path(sys.argv[3])

    # Load original document
    print(f"Loading original: {original_path}")
    original_parsed = load_json(original_path)

    # Load analysis (for consistency checking)
    analysis_path = results_dir.parent / 'analysis.json'
    if analysis_path.exists():
        analysis = load_json(analysis_path)
    else:
        analysis = {'defined_terms': {'target': []}}

    # Load section results
    print(f"Loading section results from: {results_dir}")
    section_results = []
    for result_file in sorted(results_dir.glob('section_*_result.json')):
        section_results.append(load_json(result_file))
    print(f"Loaded {len(section_results)} section results")

    output_dir.mkdir(parents=True, exist_ok=True)

    print("\n=== Phase 4: Consistency Check & Assembly ===\n")

    # 4A: Process deferred modifications
    print("4A: Processing deferred modifications...")
    deferred_path = results_dir / 'deferred_modifications.json'
    deferred_mods = []
    if deferred_path.exists():
        deferred_data = load_json(deferred_path)
        deferred_mods = deferred_data.get('modifications', [])
    print(f"  Found {len(deferred_mods)} deferred modifications")

    # Assemble revised content
    print("\nAssembling revised content...")
    revised_content, changes_log = assemble_revised_content(original_parsed, section_results)
    print(f"  Applied {len(changes_log)} changes")

    # Process deferred modifications
    if deferred_mods:
        processor = DeferredModificationProcessor(revised_content, deferred_mods, analysis)
        revised_content, applied_mods = processor.process_all()
        save_json({'applied': applied_mods}, output_dir / 'deferred_modifications_applied.json')

    # 4B: Consistency verification
    print("\n4B: Running consistency checks...")
    checker = ConsistencyChecker(revised_content, analysis)
    issues = checker.check_all()

    print(f"  Found {len(issues)} potential issues:")
    for issue in issues:
        severity_icon = {'error': '❌', 'warning': '⚠️', 'info': 'ℹ️'}.get(issue['severity'], '•')
        print(f"    {severity_icon} [{issue['type']}] {issue['description']}")

    save_json({'issues': issues}, output_dir / 'consistency_issues.json')

    # 4C: Final corrections (flagged for review if substantive)
    print("\n4C: Final corrections...")
    error_issues = [i for i in issues if i['severity'] == 'error']
    if error_issues:
        print(f"  ⚠️ {len(error_issues)} error-level issues require review before finalizing")
    else:
        print("  ✓ No critical issues found")

    # 4D: Assembly and output
    print("\n4D: Generating outputs...")

    # Context
    context = {
        'representation': analysis.get('parties', {}).get('client', 'seller')
    }

    # Generate revised.json
    revised_json = build_revised_json(original_parsed, revised_content, context)
    save_json(revised_json, output_dir / 'revised.json')
    print(f"  ✓ revised.json")

    # Generate manifest
    generate_manifest(revised_content, changes_log, context, output_dir / 'manifest.md')
    print(f"  ✓ manifest.md")

    print(f"\n=== Phase 4 Complete ===")
    print(f"Output directory: {output_dir}")
    print(f"Total changes applied: {len(changes_log)}")
    print(f"Consistency issues: {len(issues)} ({len(error_issues)} errors)")

    return {
        'revised_content': revised_content,
        'changes_log': changes_log,
        'issues': issues
    }


if __name__ == "__main__":
    main()
